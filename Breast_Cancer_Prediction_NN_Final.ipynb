{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb1be5ad-27d6-48cc-99e7-2a87e831a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import torch \n",
    "import torch.nn as nn # import neural network module\n",
    "import torch.optim as optim # import optimizer \n",
    "from sklearn.datasets import load_breast_cancer # use premade dataset to practice training and finetuning model and hyperparameters\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3a898cd-efdc-4921-941c-0ce1ceeeebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the breast cancer dataset from sklearn's built-in datasets\n",
    "# this dataset is a numerical dataset, therefore processing the units related to this set will take far less time than datasets that are primarily images\n",
    "data = load_breast_cancer()\n",
    "X = data.data # all features\n",
    "y = data.target # contains targets, benign vs malignant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77e34a5c-9530-4416-b019-ccbc3bf98a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation sets (80% training, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# training data in x train and corresponding labels in y train works similarly for x val and y val\n",
    "# total will be out train and test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f80bf3a1-0334-4079-943d-669a23f8db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the data to have a mean of 0 and a standard deviation of 1\n",
    "# for linear models standardized data is expected therefore plotting and standardizing is required\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # fit on training data then transform, test data should be scaled based on standardized training data\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01dc593a-4575-4d70-a8bb-a6640779ebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the numpy arrays to PyTorch tensors and move them to the appropriate device \n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "# note: tensor is a three dimensional matrix which is a two dim row/col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18d34f93-8f9f-4c39-b440-15c97634af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a neural network class with additional layers, batch normalization, and dropout - FNN setup given non spatial dataset - numerical setup\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # first fully connected layer\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)  # batch normalization layer\n",
    "        self.relu1 = nn.ReLU()  # ReLU activation function\n",
    "        self.dropout1 = nn.Dropout(0.5)  # dropout layer to prevent overfitting\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)  # second fully connected layer\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size // 2)  # batch normalization layer\n",
    "        self.relu2 = nn.ReLU()  # ReLU activation function\n",
    "        self.dropout2 = nn.Dropout(0.5)  # dropout layer to prevent overfitting\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_size // 2, output_size)  # Output layer\n",
    "        self.sigmoid = nn.Sigmoid()  # sigmoid activation function for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4f10d15-78de-40a0-a10d-c53001ebd14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters for the neural network\n",
    "input_size = X_train.shape[1]  # number of input features\n",
    "hidden_size = 128  # number of neurons in the first hidden layer\n",
    "output_size = 1  # output size for binary classification\n",
    "learning_rate = 0.001  # learning rate for the optimizer\n",
    "num_epochs = 100  # number of epochs to train the model\n",
    "patience = 10  # patience for early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efee286e-5ef9-4d00-b98d-ce14f0f94712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the neural network and move it to the appropriate device\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "062437e8-1b83-4744-a703-169df796bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function and the optimizer\n",
    "criterion = nn.BCELoss()  # binary cross-entropy Loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # adam optimizer\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)  # learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4462e83-b625-47e3-a4e5-b2d4aee6ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping criteria\n",
    "best_loss = np.inf  # initialize the best loss as infinity\n",
    "early_stop_count = 0  # counter for early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62aa8b7e-c262-464a-844a-61b6e42e7371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.4011, Train Accuracy: 89.89%, Val Loss: 0.4238, Val Accuracy: 94.74%\n",
      "Current learning rate: 0.001\n",
      "Epoch [20/100], Loss: 0.2924, Train Accuracy: 93.85%, Val Loss: 0.2549, Val Accuracy: 95.61%\n",
      "Current learning rate: 0.001\n",
      "Epoch [30/100], Loss: 0.2301, Train Accuracy: 95.60%, Val Loss: 0.1933, Val Accuracy: 96.49%\n",
      "Current learning rate: 0.001\n",
      "Epoch [40/100], Loss: 0.1897, Train Accuracy: 97.14%, Val Loss: 0.1693, Val Accuracy: 96.49%\n",
      "Current learning rate: 0.001\n",
      "Epoch [50/100], Loss: 0.1569, Train Accuracy: 97.80%, Val Loss: 0.1537, Val Accuracy: 96.49%\n",
      "Current learning rate: 0.001\n",
      "Epoch [60/100], Loss: 0.1333, Train Accuracy: 98.46%, Val Loss: 0.1407, Val Accuracy: 95.61%\n",
      "Current learning rate: 0.001\n",
      "Epoch [70/100], Loss: 0.1089, Train Accuracy: 98.24%, Val Loss: 0.1281, Val Accuracy: 95.61%\n",
      "Current learning rate: 0.001\n",
      "Epoch [80/100], Loss: 0.0974, Train Accuracy: 98.46%, Val Loss: 0.1177, Val Accuracy: 95.61%\n",
      "Current learning rate: 0.001\n",
      "Epoch [90/100], Loss: 0.0847, Train Accuracy: 98.90%, Val Loss: 0.1099, Val Accuracy: 95.61%\n",
      "Current learning rate: 0.001\n",
      "Epoch [100/100], Loss: 0.0803, Train Accuracy: 98.46%, Val Loss: 0.1043, Val Accuracy: 95.61%\n",
      "Current learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "# train the model with early stopping and learning rate scheduling\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # set the model to training mode\n",
    "    optimizer.zero_grad()  # zero the gradients\n",
    "    outputs = model(X_train)  # forward pass\n",
    "    loss = criterion(outputs, y_train.view(-1, 1))  # calculate loss\n",
    "    loss.backward()  # backward pass\n",
    "    optimizer.step()  # update model parameters\n",
    "\n",
    "    # calculate training accuracy\n",
    "    with torch.no_grad():\n",
    "        predicted_train = outputs.round()  # round the outputs to get binary predictions\n",
    "        correct_train = (predicted_train == y_train.view(-1, 1)).float().sum()  # calculate correct predictions\n",
    "        train_accuracy = correct_train / y_train.size(0)  # calculate training accuracy\n",
    "    \n",
    "    # validation step\n",
    "    model.eval()  # set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val)  # forward pass\n",
    "        val_loss = criterion(val_outputs, y_val.view(-1, 1))  # Calculate validation loss\n",
    "        predicted_val = val_outputs.round()  # Round the outputs to get binary predictions\n",
    "        correct_val = (predicted_val == y_val.view(-1, 1)).float().sum()  # Calculate correct predictions\n",
    "        val_accuracy = correct_val / y_val.size(0)  # Calculate validation accuracy\n",
    "    \n",
    "    scheduler.step(val_loss)  # step the learning rate scheduler\n",
    "\n",
    "    # Check for early stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss  # Update the best loss\n",
    "        early_stop_count = 0  # Reset the early stop count\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        early_stop_count += 1  # increment the early stop count\n",
    "\n",
    "    if early_stop_count >= patience:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    # print training and validation metrics every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Train Accuracy: {train_accuracy.item() * 100:.2f}%, \"\n",
    "              f\"Val Loss: {val_loss.item():.4f}, Val Accuracy: {val_accuracy.item() * 100:.2f}%\")\n",
    "        print(f\"Current learning rate: {scheduler.get_last_lr()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "432c7c8e-7342-45d1-a6f0-b7ddf8f265e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 99.12%\n",
      "Accuracy on test data: 95.61%\n"
     ]
    }
   ],
   "source": [
    "# load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# evaluate the model on the training data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_train)\n",
    "    predicted = outputs.round()\n",
    "    correct = (predicted == y_train.view(-1, 1)).float().sum()\n",
    "    accuracy = correct / y_train.size(0)\n",
    "    print(f\"Accuracy on training data: {accuracy.item() * 100:.2f}%\")\n",
    "\n",
    "# evaluate the model on the test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_val)\n",
    "    predicted = outputs.round()\n",
    "    correct = (predicted == y_val.view(-1, 1)).float().sum()\n",
    "    accuracy = correct / y_val.size(0)\n",
    "    print(f\"Accuracy on test data: {accuracy.item() * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
